{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE6003_assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3TcVcwuYZYl81HCNejfCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/CE6003/blob/master/CE6003_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-nEXis_gaV"
      },
      "source": [
        "# Assignment 2: Ships in Satellite Imagery\n",
        "In this assignment we will build & train a classical object detector using the Historgram of Gradients (HoG) Descriptor. We will use this object detector to solve the problem of idenifying ships in satellite imagry using the dataset https://www.kaggle.com/rhammell/ships-in-satellite-imagery.  This dataset consists of images of ships and images of NOT ships and is therefore a binary classification problem. The NOT ship categroies include partial ships and also linear image features such as roads, jetty's etc that may be confused with ships.   \n",
        "\n",
        "In this assignment you will write code for 3 main tasks (in numbered sections below) in order to the build a classical sliding window object detector:\n",
        "\n",
        "\n",
        "1.   Create an appropriate HoG descriptor for the training images.\n",
        "2.   Train a Support Vector Machine classifer to perform binary classification on the training set (after the training set is converted)\n",
        "3. Create a sliding window detector to find ships in larger satellite image scenes.\n",
        "\n",
        "This assignment demonstrates that classical image processing techniques may be used in preference to deep learning for certain applications. Particularly those that are limited in computational power or require fast execution. \n",
        "\n",
        "![Ships in Satellite imagery](https://i.imgur.com/tLsSoTz.png)\n",
        "\n",
        "This lab uses a kaggle dataset. Please read the [PDF](https://github.com/tonyscan6003/CE6003/blob/master/images/CE6003_kaggle_data_instructions.pdf) detailing how to setup your PC so that you can use a kaggle dataset within Colab. This assigment contains all the code necessary to read the downloaded kaggle files and create image and label arrays with test and train splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XQ_j7f8l0X0"
      },
      "source": [
        "# Dowload Kaggle Dataset.\n",
        "The following code cells will download the kaggle ship-in-satellite-imagery dataset and unzip the file. (Ensure you have a valid kaggle API token stored on your PC as described in the PDF.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxjaR3if0Xr5",
        "outputId": "92d1619b-ee8b-43e6-a570-b29cdf23258c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "\n",
        "! pip install -q kaggle==1.5.6\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-72be84a0-b9ea-4f05-aafa-1f84d6b58467\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-72be84a0-b9ea-4f05-aafa-1f84d6b58467\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tonys76\",\"key\":\"9d78e0a6d1e3074e01b5adf719084c98\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBtXYJft38r7"
      },
      "source": [
        "# Use this only if have problems with stale .json file\n",
        "#!rm kaggle.json\n",
        "#!rm ~/.kaggle/kaggle.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjUdKX-ut1eq",
        "outputId": "46d16a83-60b3-4bf6-fb1c-4d8435ce0570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Choose the kaggle.json file that you downloaded\n",
        "! mkdir ~/.kaggle\n",
        "#Make directory named kaggle and copy kaggle.json file there.\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "#Change the permissions of the file.\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Load dataset\n",
        "! kaggle datasets download rhammell/ships-in-satellite-imagery\n",
        "\n",
        "!unzip ships-in-satellite-imagery.zip > /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ships-in-satellite-imagery.zip to /content\n",
            " 98% 181M/185M [00:02<00:00, 83.3MB/s]\n",
            "100% 185M/185M [00:02<00:00, 89.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_rx0kwv8pRA"
      },
      "source": [
        "#HouseKeeping\n",
        " Import packages, helper functions to read from URL and import images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69GNKkotqa3n"
      },
      "source": [
        "\n",
        "from skimage import feature\n",
        "from skimage import exposure\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import glob\n",
        "import urllib.request\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def url_to_image(url):\n",
        "  \tresp = urllib.request.urlopen(url)\n",
        "  \ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  \ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "  \ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "  \treturn temp_image\n",
        "\n",
        "def read_image(image_url):\n",
        "    image = url_to_image(image_url)\n",
        "    x,y,z = np.shape(image)\n",
        "    # Image is scaled to reduce computation time\n",
        "    image = cv2.resize(image, dsize=(int(y/sf), int(x/sf)), interpolation=cv2.INTER_CUBIC)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GpVSarS6j6i",
        "outputId": "b719cd29-6646-454b-bf3e-ccd1e520314d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read downloaded file list: Files are comprised of positive (ship) and negaitive (Not ship) examples\n",
        "\n",
        "pos_img_list = glob.glob('shipsnet/shipsnet/1_*.png')\n",
        "neg_img_list = glob.glob('shipsnet/shipsnet/0_*.png')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rjPcY4jFTq"
      },
      "source": [
        "# Read Data \n",
        "Read data from unzipped downloaded files and create the Training and Test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKG0fPa1mw25"
      },
      "source": [
        "# Set variables\n",
        "n = 80 # number of x,y pixels in this image.\n",
        "n_pts = 4000 # number of points to use (are 4000 in dataset, 1000 positive 3000 negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjLKcqGqCkOg"
      },
      "source": [
        "\n",
        "\n",
        "def read_images(img_list):\n",
        "   train_pos = np.empty([1,n*n])\n",
        "   # Function to read images from file list \n",
        "   for i in range(len(img_list)): \n",
        "      img_path = img_list[i]\n",
        "      img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      #  store image as row in array\n",
        "      train_pos = (np.concatenate((train_pos,np.reshape(img, (1, n*n)))) if i>0 else  np.reshape(img, (1, n*n)))\n",
        "   return train_pos "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC2_IZ6DHSzr"
      },
      "source": [
        "# Read positive and negative shp images into arrays \n",
        "pos_img = read_images(pos_img_list)\n",
        "\n",
        "neg_img = read_images(neg_img_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoBo21EGQF9T"
      },
      "source": [
        "# create corresponding arrays of labels\n",
        "y_pos_labels = np.ones(len(pos_img_list))\n",
        "y_neg_labels = np.zeros(len(neg_img_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTynVmwVJ4cH"
      },
      "source": [
        "# Combine positive and negative images into one dataset. \n",
        "dataset_img = np.concatenate((pos_img[0:1000,:],neg_img[0:3000,:]))\n",
        "dataset_labels = np.append(y_pos_labels,y_neg_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO-pAgIK8IEl"
      },
      "source": [
        "# Training examples\n",
        "n_train = int(0.7*n_pts)\n",
        "\n",
        "# Divide into training and test sets with labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_img, dataset_labels, train_size=n_train,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=dataset_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VyzlLQi37Gt"
      },
      "source": [
        "# Plot some examples from the training data\n",
        "n_plots = 10 # number of plots\n",
        "f, axarr = plt.subplots(1,n_plots,figsize=(12,20))\n",
        "for i in range(0,n_plots,1):\n",
        "\n",
        "   axarr[i].imshow( np.reshape(X_train[i,0:n*n], (n, n)) ,'gray')\n",
        "   axarr[i].axis('off')\n",
        "   axarr[i].title.set_text(y_train[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweWe6maFlnC"
      },
      "source": [
        "#1. HoG Descriptor\n",
        "In order to obtain the HoG descriptor on each of the image patches during training we use the `feature.hog` command from [skimage](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html). \n",
        "\n",
        "As in the pedestran detection case (see lesson 4 weeks 10 & 11) we will use the default HoG parameters e.g. 9 orientations in each histogram, 8 x 8 pixels in the cell and 2 x 2 cells per block are used. The size of the detector window in this case is chosen to be the same as the image size = 80 x 80 pixels. This is different to the pedestrian detector window size seen in the lessons.\n",
        "\n",
        "Assuming that the blocks overlap:  \n",
        "*   Calculate how many overlapping blocks are in the 80 x 80 detector window\n",
        "*   Using the number of overlapping blocks and the other information above, determine the length of the HoG feature descriptor in this case.\n",
        "*   Insert your value for the the length of the HoG feature descriptor in the code cell below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIt6nshOsC7R"
      },
      "source": [
        "## INSERT YOUR VALUE FOR n_hog HERE\"\n",
        "n_hog = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g4HYLsH76jY"
      },
      "source": [
        "# Function that outputs HoG descriptor\n",
        "def hog_descriptor(img):\n",
        "    # convert vector to img for HoG processing\n",
        "    H = feature.hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
        "\t  cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\",\n",
        "    visualize=False)\n",
        "    return H\n",
        "\n",
        "# Function that takes the training/test data and returns array of descriptors\n",
        "def hog_des_arr(img_arr):\n",
        "    img_des=np.zeros((len(img_arr),n_hog)) # Empty descriptor array to hold values\n",
        "    i = 0                                  #pointer\n",
        "    for img in img_arr:\n",
        "      img = np.reshape(img[0:n*n], (n, n))  # convert training data vectors to rectangular image\n",
        "      img_des[i,0:n_hog] = hog_descriptor(img)\n",
        "      i=i+1\n",
        "    return img_des"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD69sYw2Ho0a"
      },
      "source": [
        "We then convert each of the image patches to it's HoG descriptor. We assign each of these descriptor vectors to the 2D arrays `X_train_des` and `X_test_des` which are now our training & test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvx-F_3yDisD"
      },
      "source": [
        "X_train_des = hog_des_arr(X_train)\n",
        "X_test_des = hog_des_arr(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lZjO34mIP8m"
      },
      "source": [
        "print(np.shape(X_train))\n",
        "print(np.shape(X_train_des))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbztAGYEEKy"
      },
      "source": [
        " # 2. Training & Test SVM Classifier\n",
        " In this section we will train a support vector machine on the data we have converted to it's HoG descriptor. Using descriptors improves classification accuracy compared to using the raw image pixels. \n",
        "\n",
        "In the code cell below utilise the [sklearn support vector machine](https://scikit-learn.org/stable/modules/svm.html) object and methods to train a support vector machine on the labelled trainig data (X_train_des,y_train)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxVBLmRYDlD7"
      },
      "source": [
        "\n",
        "\n",
        "# Setup Classifier\n",
        "## YOUR CODE HERE ##\n",
        "\n",
        "#Train on Data\n",
        "## YOUR CODE HERE ##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnZhSF6fJNkL"
      },
      "source": [
        "Test your classifier using the test dataset and obtain a [classification report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgYHgfaccgRd"
      },
      "source": [
        "# predict output labels.\n",
        "y_pred = ## YOUR CODE HERE ##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxBiaVG8upPa"
      },
      "source": [
        "# Classification Report\n",
        "## YOUR CODE HERE ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ9M3-jQIJxj"
      },
      "source": [
        "We can plot some example from the test set using the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSz1bYwKAinm"
      },
      "source": [
        "\n",
        "\n",
        "n_plots = 30 # number of plots\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "for i in range(100):\n",
        "    ax = fig.add_subplot(10, 20, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow( np.reshape(X_test[i,0:n*n], (n, n)) ,'gray')\n",
        "    color = ('black' if y_pred[i] == y_test[i] else 'red')\n",
        "    ax.set_title(y_test[i],\n",
        "                 fontsize='small', color=color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmI262eY0ZQi"
      },
      "source": [
        "# 3. Use Classifier as a basic object Detector with a sliding window.\n",
        "Now that we have a trained SVM and HoG descriptor, these components can be used as the basis of a sliding window detector. The sliding window can be used to automatically detect & mark the location of ships in a large satellite image view.\n",
        "\n",
        "**Import scene:** We can perform object detection of ships in the scene imported in the code cell below. You can use grey scale version of the image gray_img in your sliding window detector, bounding boxes can be applied to the colour version of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5fFqT5oA1Jd"
      },
      "source": [
        "\n",
        "img_path = 'scenes/scenes/sfbay_3.png'\n",
        "col_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "gray_img = cv2.cvtColor(col_img, cv2.COLOR_BGR2GRAY)\n",
        "plt.imshow( gray_img ,'gray')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LZ4JEJxLH2A"
      },
      "source": [
        "In the code cell below you can code the sliding window function `sliding_window` which operates on the scene image. You can run your detector over the entire image pixel range, there is no need to make any special accomodation for the image border. The function outputs the array m x 4, where m is the number of bounding boxes detected.\n",
        "\n",
        "\n",
        "*Step Size*\n",
        "> The sliding window which is the same size as our training images `n` = 80, is moved around the image according to the value of the parameter `step_size` which you can set to an optimimum size. Larger step values may cause an object to be missed, however using a larger step size will increase the speed of execution which is useful to debug your algorithm. When a smaller step size is used, multiple detections of the same object can occur.\n",
        "\n",
        "*Coordinate Systems*\n",
        "\n",
        "> The greyscale input image that we are applying our sliding window detector to is a matrix. The elements of this matrix are accessed as (row,column). In order to successfully plot the bouding box using OpenCV rectangle commands, the bounding boxes must be output in a format [x1,y1,x2,y2] where x1,y1 correspond to the top left hand side corner of the bouding box and x2,y2 the bottom right corner. Thus the x coordinate of the bounding box corresponds to the column of the matrix and the y coordinate the rows. The origin (0,0) in the coordinate system used by OpenCV is the top left of the image.\n",
        "\n",
        "*Non Maximal Supression*\n",
        "\n",
        ">  We can use the `non_max_supression` function from the `imutils` package to remove overlapping detections. (This is already implemented in the code cell below that displays the bouding boxes you find)  \n",
        "\n",
        "*Image Pyramid*\n",
        "\n",
        "> We will not include a image pyramid in our object detector. As the test scene images are taken from fixed position satellite images similar to the training patches, there is no need to adjust the scale of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjML4kFnDFst"
      },
      "source": [
        "# Detection window size and stride\n",
        "windowSize = (n,n)\n",
        "step_size = 10\n",
        "\n",
        "\n",
        "### COMPLETE THE SLIDING WINDOW FUNCTION HERE ###\n",
        "\n",
        "def sliding_window(image):\n",
        "   \"\"\"\n",
        "    image: input satellite image of arbitary size\n",
        "    found_boxes: m x 4 array of output bounding boxes.\n",
        "                 Each row corresponds to a bounding box [x1,y2,x2,y2] \n",
        "                 (see note on coordinate system above)\n",
        "   \"\"\"\n",
        "\n",
        "   return found_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT3fdWZiY2BM"
      },
      "source": [
        "You can run your function to output all the bounding boxes from the detections made by the classifier in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z871JVYN-FpR"
      },
      "source": [
        "\n",
        "found_boxes = sliding_window(gray_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MmRxfyhZOfk"
      },
      "source": [
        "The non_max_supression function from the imutils package is used to remove overlapping boxes. The boxes are overlaid on the color version of the satellite image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRB7rvbv0sI8"
      },
      "source": [
        "from imutils.object_detection import non_max_suppression\n",
        "\n",
        "#Bounding box parameters\n",
        "greenColor = (0, 255, 0)\n",
        "lineThickness = 4\n",
        "\n",
        "# run non-max suppression on these based on an overlay op 65%\n",
        "nmsBoundingBoxes = non_max_suppression(found_boxes, probs=None, overlapThresh=0.65)\n",
        "\n",
        "print (\"Before suppression, we had {} bounding boxes, after suppression we have {}\".format(len(found_boxes), len(nmsBoundingBoxes)))\n",
        "\n",
        "greenColor = (0, 255, 0)\n",
        "lineThickness = 1\n",
        "# draw the final bounding boxes\n",
        "for (xA, yA, xB, yB) in nmsBoundingBoxes:\n",
        "    cv2.rectangle(col_img, (xA, yA), (xB, yB), greenColor, lineThickness)\n",
        "\n",
        "# Plot figures\n",
        "plt.figure(figsize=(14, 10), dpi=80)\n",
        "plt.imshow(col_img, aspect='auto') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}